{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9afd5654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n"
     ]
    }
   ],
   "source": [
    "print(90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a335f4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nabeel is 27 years old.\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict\n",
    "\n",
    "class UserInfo(TypedDict):\n",
    "    \"\"\"A dictionary representing user information.\"\"\"\n",
    "    name : str\n",
    "    age : int\n",
    "\n",
    "def print_details(user: UserInfo):\n",
    "    print(f\"{user['name']} is {user['age']} years old.\")\n",
    "    \n",
    "print_details({\"name\": \"Nabeel\", \"age\": 27})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "881808c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Zain' age=17 status=None\n",
      "name='Ayaan' age=None status=None\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, field_validator\n",
    "from typing import Optional\n",
    "\n",
    "class UserInfo(BaseModel):\n",
    "    name: str\n",
    "    age: Optional[int] = None\n",
    "    status: Optional[str] = None\n",
    "\n",
    "    @field_validator(\"status\", mode=\"before\")\n",
    "    @classmethod\n",
    "    def default_status(cls, v, values):\n",
    "        if v is None:\n",
    "            age = values.get(\"age\")\n",
    "            if age is not None and age < 18:\n",
    "                return \"minor\"\n",
    "            return \"adult\"\n",
    "        return v\n",
    "print(UserInfo(name=\"Zain\", age=17))  # status='minor'\n",
    "print(UserInfo(name=\"Ayaan\"))         # status='adult'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff09dba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "from typing import Annotated, Sequence, TypedDict\n",
    "from langchain_core.messages import BaseMessage\n",
    "#from langgraph.prebuilt import add_messages\n",
    "from langgraph.graph.message import add_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a4007e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    language: str\n",
    "    tool_result: Optional[str]\n",
    "    user_name: Optional[str]\n",
    "    city: Optional[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "500c37ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def get_api_key(api_key_name):\n",
    "    return os.getenv(api_key_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "92b19606",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.3, google_api_key=get_api_key(\"GOOGLE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f635cfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LLM-Based City Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29aebf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "city_extractor_prompt = (\n",
    "    \"Extract the city name from this question: \\\"{question}\\\".\\n\"\n",
    "    \"If no city is mentioned, respond with 'None'.\"\n",
    ")\n",
    "\n",
    "def extract_city_with_llm(state: State) -> dict:\n",
    "    question = state[\"messages\"][-1].content\n",
    "    city_query = city_extractor_prompt.format(question=question)\n",
    "    response = llm.invoke([(\"human\", city_query)])\n",
    "    city = response.content.strip().replace(\".\", \"\")\n",
    "    return {\"city\": None if city.lower() == \"none\" else city}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d01f983c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Weather Tool Using State[\"city\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "146141d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, os\n",
    "\n",
    "OPENWEATHER_API_KEY = get_api_key(\"OPENWEATHERMAP_API_KEY\")\n",
    "\n",
    "def get_weather(city: str) -> str:\n",
    "    url = f\"https://api.openweathermap.org/data/2.5/weather?q={city}&appid={OPENWEATHER_API_KEY}&units=metric\"\n",
    "    r = requests.get(url)\n",
    "    if r.status_code != 200:\n",
    "        return f\"❌ Unable to fetch weather for {city}.\"\n",
    "    data = r.json()\n",
    "    desc = data['weather'][0]['description']\n",
    "    temp = data['main']['temp']\n",
    "    return f\"☀️ Weather in {city}: {desc}, {temp}°C\"\n",
    "\n",
    "def weather_tool(state: State) -> dict:\n",
    "    city = state[\"city\"] or \"Delhi\"\n",
    "    result = get_weather(city)\n",
    "    return {\"tool_result\": result}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f387240d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "def personalized_response(state: State) -> dict:\n",
    "    name = state.get(\"user_name\", \"friend\")\n",
    "    tool_result = state.get(\"tool_result\", \"\")\n",
    "    city = state.get(\"city\", \"\")\n",
    "    \n",
    "    message = f\"Hi {name} 👋, here’s the latest update:\\n{tool_result}\" if tool_result else \"Ask me about the weather!\"\n",
    "    return {\"messages\": [AIMessage(content=message)]}\n",
    "\n",
    "def name_setter(state: State) -> dict:\n",
    "    last = state[\"messages\"][-1].content.lower()\n",
    "    if \"my name is\" in last:\n",
    "        name = last.split(\"my name is\")[-1].strip().split()[0]\n",
    "        return {\"user_name\": name}\n",
    "    return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ce3fdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def router(state: State) -> str:\n",
    "    last = state[\"messages\"][-1].content.lower()\n",
    "\n",
    "    if \"my name is\" in last:\n",
    "        return \"name_setter\"\n",
    "\n",
    "    if \"weather\" in last or \"temperature\" in last:\n",
    "        return \"city_extractor\"\n",
    "\n",
    "    return \"llm_response\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cbd42b39",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'str'>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Routing\u001b[39;00m\n\u001b[32m     11\u001b[39m workflow.set_entry_point(\u001b[33m\"\u001b[39m\u001b[33mrouter\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43mworkflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_conditional_edges\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrouter\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mname_setter\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mname_setter\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcity_extractor\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcity_extractor\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mllm_response\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mllm_response\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     17\u001b[39m \u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# After setting name → go to LLM\u001b[39;00m\n\u001b[32m     20\u001b[39m workflow.add_edge(\u001b[33m\"\u001b[39m\u001b[33mname_setter\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mllm_response\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Ineuron-materials-FSDS\\Gen AI\\Learning-langgraph\\langgraph\\Lib\\site-packages\\langgraph\\graph\\state.py:522\u001b[39m, in \u001b[36mStateGraph.add_conditional_edges\u001b[39m\u001b[34m(self, source, path, path_map, then)\u001b[39m\n\u001b[32m    516\u001b[39m     logger.warning(\n\u001b[32m    517\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAdding an edge to a graph that has already been compiled. This will \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    518\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mnot be reflected in the compiled graph.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    519\u001b[39m     )\n\u001b[32m    521\u001b[39m \u001b[38;5;66;03m# find a name for the condition\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m522\u001b[39m path = \u001b[43mcoerce_to_runnable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    523\u001b[39m name = path.name \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcondition\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    524\u001b[39m \u001b[38;5;66;03m# validate the condition\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Ineuron-materials-FSDS\\Gen AI\\Learning-langgraph\\langgraph\\Lib\\site-packages\\langgraph\\utils\\runnable.py:494\u001b[39m, in \u001b[36mcoerce_to_runnable\u001b[39m\u001b[34m(thing, name, trace)\u001b[39m\n\u001b[32m    487\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m RunnableCallable(\n\u001b[32m    488\u001b[39m             thing,\n\u001b[32m    489\u001b[39m             wraps(thing)(partial(run_in_executor, \u001b[38;5;28;01mNone\u001b[39;00m, thing)),  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    490\u001b[39m             name=name,\n\u001b[32m    491\u001b[39m             trace=trace,\n\u001b[32m    492\u001b[39m         )\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(thing, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mRunnableParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthing\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    496\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    497\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected a Runnable, callable or dict.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    498\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInstead got an unsupported type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(thing)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    499\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Ineuron-materials-FSDS\\Gen AI\\Learning-langgraph\\langgraph\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3579\u001b[39m, in \u001b[36mRunnableParallel.__init__\u001b[39m\u001b[34m(self, steps__, **kwargs)\u001b[39m\n\u001b[32m   3576\u001b[39m merged = {**steps__} \u001b[38;5;28;01mif\u001b[39;00m steps__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m   3577\u001b[39m merged.update(kwargs)\n\u001b[32m   3578\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3579\u001b[39m     steps__={key: \u001b[43mcoerce_to_runnable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key, r \u001b[38;5;129;01min\u001b[39;00m merged.items()}\n\u001b[32m   3580\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Ineuron-materials-FSDS\\Gen AI\\Learning-langgraph\\langgraph\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5930\u001b[39m, in \u001b[36mcoerce_to_runnable\u001b[39m\u001b[34m(thing)\u001b[39m\n\u001b[32m   5925\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\u001b[33m\"\u001b[39m\u001b[33mRunnable[Input, Output]\u001b[39m\u001b[33m\"\u001b[39m, RunnableParallel(thing))\n\u001b[32m   5926\u001b[39m msg = (\n\u001b[32m   5927\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected a Runnable, callable or dict.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   5928\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInstead got an unsupported type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(thing)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   5929\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m5930\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n",
      "\u001b[31mTypeError\u001b[39m: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'str'>"
     ]
    }
   ],
   "source": [
    "workflow = StateGraph(State)\n",
    "\n",
    "# Nodes\n",
    "workflow.add_node(\"router\", RunnableLambda(router))\n",
    "workflow.add_node(\"name_setter\", RunnableLambda(name_setter))\n",
    "workflow.add_node(\"city_extractor\", RunnableLambda(extract_city_with_llm))\n",
    "workflow.add_node(\"weather\", RunnableLambda(weather_tool))\n",
    "workflow.add_node(\"llm_response\", RunnableLambda(personalized_response))\n",
    "\n",
    "# Routing\n",
    "workflow.set_entry_point(\"router\")\n",
    "\n",
    "workflow.add_conditional_edges(\"router\", {\n",
    "    \"name_setter\": \"name_setter\",\n",
    "    \"city_extractor\": \"city_extractor\",\n",
    "    \"llm_response\": \"llm_response\"\n",
    "})\n",
    "\n",
    "# After setting name → go to LLM\n",
    "workflow.add_edge(\"name_setter\", \"llm_response\")\n",
    "\n",
    "# After extracting city → get weather → reply\n",
    "workflow.add_edge(\"city_extractor\", \"weather\")\n",
    "workflow.add_edge(\"weather\", \"llm_response\")\n",
    "\n",
    "workflow.set_finish_point(\"llm_response\")\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930bff46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9616d334",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
